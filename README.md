# VLIW ASM OPT



## Overview

This repository is a personal project aimed at researching and understanding low-level optimization techniques for VLIW (Very Long Instruction Word) architectures. It focuses on manual reproduction of optimizations typically handled automatically by compilers, such as instruction scheduling, loop unrolling, and register allocation, to gain deeper insights into the architecture's characteristics.
> ⚠️ This implementation has not been fully validated on actual hardware, and thus, performance and behavioral correctness are not guaranteed.


## Development Environment

- **IDE**: Code Composer Studio v7.4.0.00015  
- **Target**: Generic C64X+ Device  



## Algorithm Design
The target algorithm is a 28×28 convolution operation. This computation is commonly used in image processing and neural network preprocessing. Due to its computational intensity, it is an ideal candidate to showcase the benefits of optimization. In this implementation, bottleneck operations are isolated into functions to enable instruction-level optimization.

## Implementation Flow

### Intrinsic 

Identify areas within the algorithm suitable for SIMD and apply intrinsic functions to those sections. Additionally, loop unrolling is applied to small, fixed-length loops as part of the optimization.




- SIMD<br>
    From the linear assembler generated by the C compiler, the instruction set used is extracted. This serves as the foundation for selecting VLIW-appropriate instructions and constructing an instruction scheduling strategy.

For this algorithm, the DOTPU4 SIMD instruction (which performs four parallel 8bit×8bit multiply-accumulate operations) is applied. The 3×3 kernel is expanded to 3×4 using a preset table that pads the extra element with zero. This allows effective use of DOTPU4 without affecting the calculation results.

![SIMD](https://github.com/user-attachments/assets/63149fca-079e-4926-a85e-2bb8c858632e)


- Loop Unrolling<br>
    When loop iteration counts are fixed, the loop body is expanded (unrolled) for multiple iterations to reduce the overhead of loop control. This reduces the number of branch instructions and improves pipeline efficiency, particularly effective in small fixed-count loops such as the 3×3 convolution loop used here.

Example code before and after unrolling:


  - Before Unrolling:
    ```c
    for (ky = -1; ky <= 1; ky++) {
        for (kx = -1; kx <= 1; kx++) {
            int pixel = in[(y + ky) * W + (x + kx)];
            int coeff = k[(ky + 1) * 3 + (kx + 1)];
            sum += pixel * coeff;
        }
    }
    if (sum < 0) sum = 0;
    if (sum > 255) sum = 255;
    out[y * W + x] = sum;
    ```

  - After Unrolling:
    
    ```c
    unsigned char *in_ptr = (unsigned char *)in;
    unsigned int row3_1 = _mem4((void *)(in_ptr + (y-1)*W + (x-1))) & 0xFFFFFF;
    unsigned int row6_4 = _mem4((void *)(in_ptr + (y  )*W + (x-1))) & 0xFFFFFF;
    unsigned int row9_7 = _mem4((void *)(in_ptr + (y+1)*W + (x-1))) & 0xFFFFFF;

    int sum = 0;
    sum += _dotpsu4(k3_1, row3_1);
    sum += _dotpsu4(k6_4, row6_4);
    sum += _dotpsu4(k9_7, row9_7);

    if (sum < 0)   sum = 0;
    if (sum > 255) sum = 255;

    out[y * W + x] = sum;
    ```




### Linear Assembler
Register allocation to functional units is delegated to the compiler, including selection of instruction sets and scheduling.

- Register Assignment<br>
    Function arguments, return values, return addresses, and local variables are assigned to appropriate registers following the ABI specification.
    ![レジスタ](https://github.com/user-attachments/assets/c2f9b3a0-5838-4c07-b363-d40d4d56bbc7)

- Unit Assignment / Delay Slots<br>
    Instructions are assigned to appropriate units—D (data), S (scalar), L (load/store), and M (MAC)—while considering instruction latencies and delay slots. Instructions are initially arranged sequentially without optimization to establish a baseline.
![image](https://github.com/user-attachments/assets/f6a0a5b1-a5d6-45b9-adc3-633430141587)


### Assembler Step 2
Instruction order within loops is optimized to achieve dense and efficient scheduling while accounting for data dependencies. This produces bundles of VLIW instructions that can execute in parallel.
Distinguish between usage of A and B register banks and select corresponding functional units appropriately. Cross-path instruction selection between register banks is critical for performance. Also, redundant operations inside loops (e.g., shown in blue boxes) are moved outside to eliminate inefficiencies.

![asm1](https://github.com/user-attachments/assets/2656deb7-3d2e-4b55-a12d-205e9cc87363)


### Assembler Final

Final instruction ordering spans loop boundaries to further increase instruction density. If the optimization effect does not meet expectations, earlier steps are revisited iteratively to refine and explore better configurations.
![asm2](https://github.com/user-attachments/assets/bb87cc15-1b4d-4b3d-be28-731a0cca22f7)



## Loop Kernel Implementations (Comparison)

| Implementation | Loop Cycle Count (ii)                      | Remarks |
|--------|------------------------------|------|
| `convolution.c`   | 28 cycles or more | Scheduling failed by compiler |
| `convolution_intrinsic.c` | 23 cycle | |
| `convolution_linear_asm.sa`| 15 cycle  |  |
| `convolution_hand_asm1.asm`    | 16 cycle  |  |
| `convolution_hand_asm2.asm`    | 12 cycle  |  |
| `convolution_hand_asm3.asm`    | 8 cycle |



